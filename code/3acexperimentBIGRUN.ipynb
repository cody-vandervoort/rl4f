{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import asyncio\n",
    "import ccxt.async_support as ccxt  # Use asynchronous version\n",
    "from backtesting import Backtest, Strategy\n",
    "from backtesting.lib import resample_apply\n",
    "from colorama import Fore, Style\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random  # Needed for random sampling in agent's replay memory\n",
    "\n",
    "# Suppress warnings and logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Force TensorFlow to use CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# Ensure plots display correctly in Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Allow nested event loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# -------------------------------------\n",
    "# 1. Asynchronous Data Fetching Functions\n",
    "# -------------------------------------\n",
    "\n",
    "async def fetch_exchange_data(symbol, timeframe, start_date, end_date, limit=1000):\n",
    "    exchange = ccxt.bybit({\n",
    "        'enableRateLimit': True,  # Enable rate limiting\n",
    "    })\n",
    "    await exchange.load_markets()\n",
    "    all_ohlcv_data = []\n",
    "    since = exchange.parse8601(start_date + 'T00:00:00Z')\n",
    "    end_timestamp = exchange.parse8601(end_date + 'T23:59:59Z')\n",
    "\n",
    "    timeframe_ms = timeframe_to_milliseconds(timeframe)\n",
    "\n",
    "    with tqdm(total=1, desc=f\"Fetching data for {symbol}\", leave=False) as pbar:\n",
    "        while since < end_timestamp:\n",
    "            try:\n",
    "                ohlcv_data = await exchange.fetch_ohlcv(symbol, timeframe, since=since, limit=limit)\n",
    "                if not ohlcv_data:\n",
    "                    break\n",
    "                all_ohlcv_data.extend(ohlcv_data)\n",
    "                since = ohlcv_data[-1][0] + timeframe_ms\n",
    "                await asyncio.sleep(exchange.rateLimit / 1000)\n",
    "            except Exception:\n",
    "                break\n",
    "            pbar.update(len(ohlcv_data) / limit)\n",
    "        pbar.close()\n",
    "\n",
    "    await exchange.close()\n",
    "\n",
    "    if not all_ohlcv_data:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if no data\n",
    "\n",
    "    df = pd.DataFrame(all_ohlcv_data, columns=['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    df['Datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    return df\n",
    "\n",
    "def timeframe_to_milliseconds(timeframe):\n",
    "    \"\"\"\n",
    "    Convert a timeframe string to milliseconds.\n",
    "    \"\"\"\n",
    "    unit = timeframe[-1]\n",
    "    value = int(timeframe[:-1])\n",
    "    if unit == 'm':\n",
    "        return value * 60 * 1000\n",
    "    elif unit == 'h':\n",
    "        return value * 60 * 60 * 1000\n",
    "    elif unit == 'd':\n",
    "        return value * 24 * 60 * 60 * 1000\n",
    "    elif unit == 'w':\n",
    "        return value * 7 * 24 * 60 * 60 * 1000\n",
    "    elif unit == 'M':\n",
    "        return value * 30 * 24 * 60 * 60 * 1000  # Approximation\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown timeframe unit: {unit}\")\n",
    "\n",
    "async def fetch_all_data(symbols, timeframe, start_date, end_date):\n",
    "    tasks = []\n",
    "    for symbol in symbols:\n",
    "        task = fetch_exchange_data(symbol, timeframe, start_date, end_date)\n",
    "        tasks.append(task)\n",
    "    data_list = await asyncio.gather(*tasks)\n",
    "    # Map symbols to their respective DataFrames\n",
    "    return dict(zip(symbols, data_list))\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. Calculating Volume Profile\n",
    "# -------------------------------------\n",
    "\n",
    "def calculate_volume_profile(data):\n",
    "    \"\"\"\n",
    "    Calculate Volume Area High (VAH), Value Area Low (VAL), and Point of Control (POC) for each day.\n",
    "    \"\"\"\n",
    "    profile = pd.DataFrame(index=data.resample('1D').mean().index)\n",
    "\n",
    "    def calculate_vah_val_poc(df):\n",
    "        if df.empty or df['Volume'].sum() == 0:\n",
    "            return np.nan, np.nan, np.nan\n",
    "\n",
    "        total_volume = df['Volume'].sum()\n",
    "        price_sorted = df.sort_values(by='Close').copy()\n",
    "        price_sorted['CumVolume'] = price_sorted['Volume'].cumsum()\n",
    "\n",
    "        # Point of Control (POC): Price with the highest volume\n",
    "        poc_index = df['Volume'].idxmax()\n",
    "        poc_price = df.loc[poc_index, 'Close']\n",
    "\n",
    "        # Value Area High (VAH): Price where cumulative volume >= 68% of total volume\n",
    "        try:\n",
    "            vah_idx = price_sorted[price_sorted['CumVolume'] >= total_volume * 0.68].index[0]\n",
    "            vah_price = df.loc[vah_idx, 'High']\n",
    "        except IndexError:\n",
    "            vah_price = np.nan\n",
    "\n",
    "        # Value Area Low (VAL): Price where cumulative volume <= 32% of total volume\n",
    "        try:\n",
    "            val_idx = price_sorted[price_sorted['CumVolume'] <= total_volume * 0.32].index[-1]\n",
    "            val_price = df.loc[val_idx, 'Low']\n",
    "        except IndexError:\n",
    "            val_price = np.nan\n",
    "\n",
    "        return vah_price, val_price, poc_price\n",
    "\n",
    "    # Apply the function to each day\n",
    "    result = data.groupby(pd.Grouper(freq='1D')).apply(calculate_vah_val_poc)\n",
    "    profile[['VAH', 'VAL', 'POC']] = pd.DataFrame(result.tolist(), index=profile.index)\n",
    "\n",
    "    return profile\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Reinforcement Learning Agent\n",
    "# -------------------------------------\n",
    "\n",
    "class RLAgent:\n",
    "    def __init__(self, state_size, action_size, hu=16, lr=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = self._build_model(hu, lr)\n",
    "\n",
    "    def _build_model(self, hu, lr):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hu, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(hu, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=lr))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        # Epsilon-greedy action selection\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size=16):\n",
    "        # Experience replay to train the model\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        # Decrease exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. Training Function for the RL Agent\n",
    "# -------------------------------------\n",
    "\n",
    "def train_agent(agent, data):\n",
    "    print(f\"{Fore.GREEN}Training the RL agent...{Style.RESET_ALL}\")\n",
    "    episodes = 1  # Single pass over the data\n",
    "    batch_size = 16\n",
    "    data_length = len(data)\n",
    "    training_data = data.copy()\n",
    "\n",
    "    # Reset the environment\n",
    "    position_flag = 0  # 1 if in position, 0 otherwise\n",
    "    last_price = None\n",
    "    total_rewards = []\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = None\n",
    "        total_reward = 0\n",
    "        data_iter = tqdm(range(data_length - 1), desc=f\"Episode {e+1}/{episodes}\", leave=False)\n",
    "        for idx in data_iter:\n",
    "            # Construct state\n",
    "            current_row = training_data.iloc[idx]\n",
    "            next_row = training_data.iloc[idx + 1]\n",
    "            current_price = current_row['Close']\n",
    "            vah = current_row['VAH']\n",
    "            val = current_row['VAL']\n",
    "            poc = current_row['POC']\n",
    "\n",
    "            if np.isnan(vah) or np.isnan(val) or np.isnan(poc):\n",
    "                continue  # Skip if data is not available\n",
    "\n",
    "            state = np.array([[current_price, vah, val, poc, position_flag]])\n",
    "            state = state / state[0][0]  # Normalize by current price\n",
    "\n",
    "            # Decide action\n",
    "            action = agent.act(state, training=True)\n",
    "\n",
    "            # Map action to trading decision\n",
    "            reward = 0\n",
    "            if action == 1 and position_flag == 0:\n",
    "                # Buy action\n",
    "                position_flag = 1\n",
    "                last_price = current_price\n",
    "            elif action == 2 and position_flag == 1:\n",
    "                # Sell action\n",
    "                reward = (current_price - last_price) / last_price\n",
    "                position_flag = 0\n",
    "                last_price = None\n",
    "            elif position_flag == 1 and last_price:\n",
    "                # Hold position\n",
    "                reward = (current_price - last_price) / last_price\n",
    "\n",
    "            # Construct next state\n",
    "            next_price = next_row['Close']\n",
    "            vah_next = next_row['VAH']\n",
    "            val_next = next_row['VAL']\n",
    "            poc_next = next_row['POC']\n",
    "            next_state = np.array([[next_price, vah_next, val_next, poc_next, position_flag]])\n",
    "            next_state = next_state / next_state[0][0]\n",
    "\n",
    "            done = idx == data_length - 2  # Last data point\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            data_iter.set_postfix({'Total Reward': f'{total_reward:.4f}', 'Epsilon': f'{agent.epsilon:.4f}'})\n",
    "        data_iter.close()\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {e+1}/{episodes}, Total Reward: {total_reward:.4f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "    print(f\"{Fore.GREEN}Training completed.{Style.RESET_ALL}\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 5. Custom Strategy Class for Testing\n",
    "# -------------------------------------\n",
    "\n",
    "class TestRLStrategy(Strategy):\n",
    "    def init(self):\n",
    "        # Initialize the trained RL agent\n",
    "        self.state_size = 5  # [Close, VAH, VAL, POC, Position]\n",
    "        self.action_size = 3  # [Hold, Buy, Sell]\n",
    "        self.agent = self._load_agent()\n",
    "\n",
    "        self.position_flag = 0  # 1 if in position, 0 otherwise\n",
    "        self.last_price = None\n",
    "\n",
    "        # Prepare data\n",
    "        self.data.VAH = self.data.VAH.ffill()\n",
    "        self.data.VAL = self.data.VAL.ffill()\n",
    "        self.data.POC = self.data.POC.ffill()\n",
    "\n",
    "    def _load_agent(self):\n",
    "        # Build the same architecture as during training\n",
    "        agent = RLAgent(state_size=self.state_size, action_size=self.action_size, hu=16)\n",
    "        # Load the trained weights\n",
    "        agent.load('rl_agent_weights.h5')\n",
    "        return agent\n",
    "\n",
    "    def next(self):\n",
    "        # Construct state\n",
    "        current_price = self.data.Close[-1]\n",
    "        vah = self.data.VAH[-1]\n",
    "        val = self.data.VAL[-1]\n",
    "        poc = self.data.POC[-1]\n",
    "\n",
    "        if np.isnan(vah) or np.isnan(val) or np.isnan(poc):\n",
    "            return  # Skip if data is not available\n",
    "\n",
    "        state = np.array([[current_price, vah, val, poc, self.position_flag]])\n",
    "        state = state / state[0][0]  # Normalize by current price\n",
    "\n",
    "        # Decide action\n",
    "        action = self.agent.act(state, training=False)\n",
    "\n",
    "        # Map action to trading decision\n",
    "        if action == 1 and not self.position_flag:\n",
    "            # Buy action\n",
    "            self.buy()\n",
    "            self.position_flag = 1\n",
    "            self.last_price = current_price\n",
    "        elif action == 2 and self.position_flag:\n",
    "            # Sell action\n",
    "            self.position.close()\n",
    "            self.position_flag = 0\n",
    "            self.last_price = None\n",
    "\n",
    "    def on_trade(self, trade):\n",
    "        # Update position flag on trade execution\n",
    "        if trade.is_closed:\n",
    "            self.position_flag = 0\n",
    "            self.last_price = None\n",
    "\n",
    "# -------------------------------------\n",
    "# 6. Main function to run everything\n",
    "# -------------------------------------\n",
    "\n",
    "async def main():\n",
    "    SYMBOLS = ['SOL/USDT']  # Adjust symbols as needed\n",
    "    timeframe = '1m'  # Adjust timeframe as needed\n",
    "    # Adjusted dates within knowledge cutoff\n",
    "    end_date = '2023-10-31'\n",
    "    start_date = (dt.datetime.strptime(end_date, '%Y-%m-%d') - dt.timedelta(weeks=2)).strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"{Fore.BLUE}Fetching data from {start_date} to {end_date} on a {timeframe} timeframe...{Style.RESET_ALL}\")\n",
    "    data_dict = await fetch_all_data(SYMBOLS, timeframe, start_date, end_date)\n",
    "\n",
    "    symbol = SYMBOLS[0]\n",
    "    data = data_dict[symbol]\n",
    "\n",
    "    if data.empty:\n",
    "        print(f\"{Fore.RED}No data fetched for {symbol}. Exiting.{Style.RESET_ALL}\")\n",
    "        return\n",
    "\n",
    "    print(f\"{Fore.BLUE}Calculating volume profile for {symbol}...{Style.RESET_ALL}\")\n",
    "    profile_data = calculate_volume_profile(data)\n",
    "    data[['VAH', 'VAL', 'POC']] = profile_data[['VAH', 'VAL', 'POC']]\n",
    "    data.ffill(inplace=True)\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Split data into training and testing sets (80% training, 20% testing)\n",
    "    split_index = int(len(data) * 0.8)\n",
    "    training_data = data.iloc[:split_index]\n",
    "    testing_data = data.iloc[split_index:]\n",
    "\n",
    "    # Build and train the agent using training data\n",
    "    state_size = 5  # [Close, VAH, VAL, POC, Position]\n",
    "    action_size = 3  # [Hold, Buy, Sell]\n",
    "    agent = RLAgent(state_size=state_size, action_size=action_size, hu=16)\n",
    "    train_agent(agent, training_data)\n",
    "\n",
    "    # Save the trained agent\n",
    "    agent.save('rl_agent_weights.h5')\n",
    "\n",
    "    # Prepare testing data for backtesting.py\n",
    "    bt_data = testing_data.copy()\n",
    "    bt_data.index.name = 'Datetime'\n",
    "    bt_data = bt_data[['Open', 'High', 'Low', 'Close', 'Volume', 'VAH', 'VAL', 'POC']]\n",
    "\n",
    "    # Run backtest with TestRLStrategy\n",
    "    bt = Backtest(\n",
    "        bt_data,\n",
    "        TestRLStrategy,\n",
    "        cash=100000,\n",
    "        commission=.002,\n",
    "        exclusive_orders=True,\n",
    "        margin=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"{Fore.GREEN}Running backtest on testing data...{Style.RESET_ALL}\")\n",
    "    stats = bt.run()\n",
    "\n",
    "    # Print the backtest results\n",
    "    print(stats)\n",
    "\n",
    "    # Plot the backtest results\n",
    "    bt.plot()\n",
    "\n",
    "# Since we are in a Jupyter notebook, we can directly await the main function\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
