import os
import math
import random
import numpy as np
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
import warnings
import asyncio
import concurrent.futures
from collections import deque
from scipy.optimize import minimize
from pylab import mpl
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import ccxt.async_support as ccxt  # Use asynchronous version
from colorama import Fore, Style
from tqdm.notebook import tqdm  # Adjusted for Jupyter notebooks

# Suppress TensorFlow and Keras logs
tf.get_logger().setLevel('ERROR')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# If nest_asyncio is not installed, install it
# !pip install nest_asyncio
import nest_asyncio
nest_asyncio.apply()  # Allow nested event loops in Jupyter

# Ensure plots display correctly in Jupyter notebooks
%matplotlib inline

warnings.filterwarnings("ignore")

plt.style.use('seaborn-v0_8')
mpl.rcParams['figure.dpi'] = 300
mpl.rcParams['savefig.dpi'] = 300
mpl.rcParams['font.family'] = 'serif'
np.set_printoptions(suppress=True)
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# -------------------------------------
# 1. Constants and Configuration
# -------------------------------------

# Simplified to use only two symbols
SYMBOLS = ['SOL/USDT', 'TON/USDT']

NUM_SYMBOLS = len(SYMBOLS)

# -------------------------------------
# 2. Asynchronous Data Fetching Functions
# -------------------------------------

async def fetch_exchange_data(symbol, timeframe, start_date, end_date, limit=1000):
    exchange = ccxt.bybit({
        'enableRateLimit': True,  # Enable rate limiting
    })
    await exchange.load_markets()
    all_ohlcv_data = []
    since = exchange.parse8601(start_date + 'T00:00:00Z')
    end_timestamp = exchange.parse8601(end_date + 'T23:59:59Z')

    timeframe_ms = timeframe_to_milliseconds(timeframe)

    while since < end_timestamp:
        try:
            ohlcv_data = await exchange.fetch_ohlcv(symbol, timeframe, since=since, limit=limit)
            if not ohlcv_data:
                break
            all_ohlcv_data.extend(ohlcv_data)
            since = ohlcv_data[-1][0] + timeframe_ms
            await asyncio.sleep(exchange.rateLimit / 1000)
        except Exception:
            break

    await exchange.close()

    if not all_ohlcv_data:
        return pd.DataFrame()  # Return empty DataFrame if no data

    df = pd.DataFrame(all_ohlcv_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
    df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
    df.set_index('datetime', inplace=True)
    df.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low',
                       'close': 'Close', 'volume': 'Volume'}, inplace=True)
    return df

def timeframe_to_milliseconds(timeframe):
    """
    Convert a timeframe string to milliseconds.
    Example: '1m' -> 60000
             '5m' -> 300000
             '1h' -> 3600000
    """
    unit = timeframe[-1]
    value = int(timeframe[:-1])
    if unit == 'm':
        return value * 60 * 1000
    elif unit == 'h':
        return value * 60 * 60 * 1000
    elif unit == 'd':
        return value * 24 * 60 * 60 * 1000
    elif unit == 'w':
        return value * 7 * 24 * 60 * 60 * 1000
    elif unit == 'M':
        return value * 30 * 24 * 60 * 60 * 1000  # Approximation
    else:
        raise ValueError(f"Unknown timeframe unit: {unit}")

async def fetch_all_data(symbols, timeframe, start_date, end_date):
    tasks = []
    for symbol in symbols:
        task = fetch_exchange_data(symbol, timeframe, start_date, end_date)
        tasks.append(task)
    data_list = await asyncio.gather(*tasks)
    # Map symbols to their respective DataFrames
    return dict(zip(symbols, data_list))

# -------------------------------------
# 3. Calculating Volume Profile
# -------------------------------------

def calculate_volume_profile(data):
    """
    Calculate Volume Area High (VAH), Value Area Low (VAL), and Point of Control (POC) for each day.
    """
    profile = pd.DataFrame(index=data.resample('1D').mean().index)

    def calculate_vah_val_poc(df):
        if df.empty or df['Volume'].sum() == 0:
            return np.nan, np.nan, np.nan

        total_volume = df['Volume'].sum()
        price_sorted = df.sort_values(by='Close').copy()
        price_sorted['CumVolume'] = price_sorted['Volume'].cumsum()

        # Point of Control (POC): Price with the highest volume
        poc_index = df['Volume'].idxmax()
        poc_price = df.loc[poc_index, 'Close']

        # Value Area High (VAH): Price where cumulative volume >= 68% of total volume
        try:
            vah_idx = price_sorted[price_sorted['CumVolume'] >= total_volume * 0.68].index[0]
            vah_price = price_sorted.loc[vah_idx, 'High']
        except IndexError:
            vah_price = np.nan

        # Value Area Low (VAL): Price where cumulative volume <= 32% of total volume
        try:
            val_idx = price_sorted[price_sorted['CumVolume'] <= total_volume * 0.32].index[-1]
            val_price = price_sorted.loc[val_idx, 'Low']
        except IndexError:
            val_price = np.nan

        return vah_price, val_price, poc_price

    # Apply the function to each day
    result = data.groupby(pd.Grouper(freq='1D')).apply(calculate_vah_val_poc)
    profile[['VAH', 'VAL', 'POC']] = pd.DataFrame(result.tolist(), index=profile.index)

    return profile

# -------------------------------------
# 4. Investing Environment and Agent
# -------------------------------------

class Investing:
    def __init__(self, symbols, data_dict, steps=50, amount=1):
        self.symbols = symbols
        self.data_dict = data_dict
        self.steps = steps
        self.initial_balance = amount
        self.portfolio_value = amount
        self.portfolio_value_new = amount
        self.action_space = action_space(len(symbols))
        self.observation_space = observation_space(len(symbols) * 2)  # Adjusted for multiple assets
        self.retrieved = True
        self._generate_data()
        self.portfolios = pd.DataFrame()
        self.episode = 0

    def _generate_data(self):
        # Prepare data for each symbol
        self.data = {}
        for symbol in self.symbols:
            df = self.data_dict[symbol]
            if df.empty:
                continue
            df = df.copy()
            df['X'] = df['Close']
            df = df[['X']]
            if len(df) < self.steps:
                continue  # Skip symbols with insufficient data
            s = random.randint(self.steps, len(df))
            df = df.iloc[s - self.steps:s]
            df = df / df.iloc[0]
            self.data[symbol] = df

    def _get_state(self):
        state = []
        dates = []
        for symbol in self.symbols:
            if symbol not in self.data or self.bar >= len(self.data[symbol]):
                state.extend([0, 0])  # Placeholder for missing data
                dates.append(None)
                continue
            Xt = self.data[symbol]['X'].iloc[self.bar]
            date = self.data[symbol].index[self.bar]
            state.extend([Xt, self.xt.get(symbol, 0)])
            dates.append(date)
        # Use the first non-None date as the current date
        date = next((d for d in dates if d is not None), None)
        return np.array(state), {'date': date}

    def seed(self, seed=None):
        if seed is not None:
            random.seed(seed)

    def reset(self):
        self.xt = {}
        self.bar = 0
        self.treward = 0
        self.portfolio_value = self.initial_balance
        self.portfolio_value_new = self.initial_balance
        self.episode += 1
        self._generate_data()
        self.state, info = self._get_state()
        return self.state, info

    def add_results(self, pl):
        df = pd.DataFrame({
            'e': self.episode,
            'date': self.date,
            'xt': [self.xt.copy()],
            'pv': self.portfolio_value,
            'pv_new': self.portfolio_value_new,
            'p&l[$]': pl,
            'p&l[%]': pl / self.portfolio_value_new * 100 if self.portfolio_value_new != 0 else 0,
        }, index=[0])
        self.portfolios = pd.concat((self.portfolios, df), ignore_index=True)

    def step(self, action):
        self.bar += 1
        self.new_state, info = self._get_state()
        self.date = info['date']
        if self.bar == 1:
            self.xt = {symbol: action[i] for i, symbol in enumerate(self.symbols)}
            pl = 0.
            reward = 0.
            self.add_results(pl)
        else:
            self.portfolio_value_new = 0
            for i, symbol in enumerate(self.symbols):
                if symbol not in self.data or self.bar >= len(self.data[symbol]):
                    continue
                prev_price = self.data[symbol]['X'].iloc[self.bar - 1]
                new_price = self.data[symbol]['X'].iloc[self.bar]
                allocation = self.xt.get(symbol, 0)
                self.portfolio_value_new += allocation * self.portfolio_value * new_price / prev_price

            pl = self.portfolio_value_new - self.portfolio_value
            pen = sum((self.xt.get(symbol, 0) - action[i]) ** 2 for i, symbol in enumerate(self.symbols))
            self.xt = {symbol: action[i] for i, symbol in enumerate(self.symbols)}
            self.add_results(pl)
            ret = self.portfolios['p&l[%]'].iloc[-1] / 100 * 252
            vol = self.portfolios['p&l[%]'].rolling(
                20, min_periods=1).std().iloc[-1] * math.sqrt(252)
            sharpe = ret / vol if vol != 0 else 0
            reward = sharpe - pen
            self.portfolio_value = self.portfolio_value_new
        if self.bar >= self.steps - 1:
            done = True
        else:
            done = False
        self.state = self.new_state
        return self.state, reward, done, False, {}

class observation_space:
    def __init__(self, n):
        self.shape = (n,)

class action_space:
    def __init__(self, n):
        self.n = n

    def seed(self, seed):
        random.seed(seed)

    def sample(self):
        rn = np.random.random(self.n)
        return rn / rn.sum()

# -------------------------------------
# 5. Investing Agent
# -------------------------------------

class InvestingAgent:
    def __init__(self, symbol, feature, n_features, env, hu=32, lr=0.00025):
        self.epsilon = 1.0
        self.epsilon_decay = 0.9975
        self.epsilon_min = 0.1
        self.memory = deque(maxlen=2000)
        self.batch_size = 32
        self.gamma = 0.5
        self.trewards = list()
        self.max_treward = -np.inf
        self.n_features = n_features
        self.env = env
        self.episodes = 0
        self._create_model(hu, lr)

    def _create_model(self, hu, lr):
        self.model = Sequential()
        self.model.add(Dense(hu, activation='relu',
                             input_dim=self.n_features))
        self.model.add(Dense(hu, activation='relu'))
        self.model.add(Dense(len(self.env.symbols), activation='linear'))
        self.model.compile(loss='mse', optimizer=Adam(learning_rate=lr))

    def _reshape(self, state):
        state = state.flatten()
        return np.reshape(state, [1, len(state)])

    def opt_action(self, state):
        bnds = len(self.env.symbols) * [(0, 1)]
        cons = [{'type': 'eq', 'fun': lambda x: x.sum() - 1}]
        def f(x):
            s = state.copy()
            for i in range(len(self.env.symbols)):
                s[0, i * 2 + 1] = x[i]  # Adjust index based on state structure
            pen = np.mean((state[0, 1::2] - x) ** 2)
            pred = self.model.predict(s, verbose=0)[0]
            return -np.sum(pred) + pen
        try:
            state = self._reshape(state)
            res = minimize(f,
                           len(self.env.symbols) * [1 / len(self.env.symbols)],
                           bounds=bnds,
                           constraints=cons,
                           options={'eps': 1e-4},
                           method='SLSQP')
            self.action = res.x
        except Exception:
            self.action = np.ones(len(self.env.symbols)) / len(self.env.symbols)
        return self.action

    def act(self, state):
        if random.random() <= self.epsilon:
            return self.env.action_space.sample()
        action = self.opt_action(state)
        return action

    def replay(self):
        batch = random.sample(self.memory, min(len(self.memory), self.batch_size))
        for state, action, next_state, reward, done in batch:
            target = reward
            if not done:
                ns = next_state.copy()
                next_action = self.opt_action(ns)
                for i in range(len(self.env.symbols)):
                    ns[0, i * 2 + 1] = next_action[i]
                target += self.gamma * np.sum(self.model.predict(ns, verbose=0)[0])
            target_f = self.model.predict(state, verbose=0)
            target_f[0] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def learn(self, episodes):
        progress_bar = tqdm(range(1, episodes + 1), desc="Training episodes", leave=True)
        for e in progress_bar:
            self.episodes += 1
            state, _ = self.env.reset()
            state = self._reshape(state)
            treward = 0
            for f in range(1, self.env.steps + 1):
                action = self.act(state)
                next_state, reward, done, trunc, _ = self.env.step(action)
                treward += reward
                next_state = self._reshape(next_state)
                self.memory.append(
                    [state, action, next_state, reward, done])
                state = next_state
                if done:
                    self.trewards.append(treward)
                    self.max_treward = max(self.max_treward, treward)
                    # Update progress bar with current rewards
                    progress_bar.set_postfix({'treward': f'{treward:.3f}', 'max': f'{self.max_treward:.3f}'})
                    break
            if len(self.memory) > self.batch_size:
                self.replay()
        print()

    def test(self, episodes, verbose=True):
        progress_bar = tqdm(range(1, episodes + 1), desc="Testing episodes", leave=True)
        for e in progress_bar:
            state, _ = self.env.reset()
            state = self._reshape(state)
            treward = 0
            for _ in range(1, self.env.steps + 1):
                action = self.opt_action(state)
                state, reward, done, trunc, _ = self.env.step(action)
                state = self._reshape(state)
                treward += reward
                if done:
                    progress_bar.set_postfix({'treward': f'{treward:.2f}'})
                    if verbose:
                        print(f"Episode {e}: Total reward = {treward:.2f}")
                    break
        print()

# -------------------------------------
# 6. Main function to run everything
# -------------------------------------

async def main():
    timeframe = '1m'  # Adjusted timeframe
    start_date = (dt.datetime.now() - dt.timedelta(weeks=30)).strftime('%Y-%m-%d')  # Use 4 weeks of data
    end_date = dt.datetime.now().strftime('%Y-%m-%d')

    data_dict = await fetch_all_data(SYMBOLS, timeframe, start_date, end_date)

    # Parallelized Volume Profile Calculations
    def process_symbol(symbol_data):
        symbol, data = symbol_data
        if data.empty:
            return symbol, data
        profile_data = calculate_volume_profile(data)
        data[['VAH', 'VAL', 'POC']] = profile_data[['VAH', 'VAL', 'POC']]
        data.ffill(inplace=True)
        return symbol, data

    # Prepare list of symbol-data pairs
    symbol_data_list = list(data_dict.items())

    # Process volume profiles in parallel with progress bar
    with concurrent.futures.ThreadPoolExecutor() as executor:
        results = list(tqdm(executor.map(process_symbol, symbol_data_list), total=len(symbol_data_list), desc="Processing symbols"))
        data_dict = dict(results)

    # Prepare the environment and agent
    investing = Investing(SYMBOLS, data_dict, steps=50, amount=1)
    agent = InvestingAgent('MultiAsset', feature=None, n_features=investing.observation_space.shape[0],
                           env=investing, hu=32, lr=0.00025)

    # Train the agent
    episodes = 5  # Reduced number of episodes
    print(f"{Fore.GREEN}Starting training of the Investing Agent...{Style.RESET_ALL}")
    agent.learn(episodes)

    # Test the agent
    print(f"{Fore.GREEN}Testing the trained Investing Agent...{Style.RESET_ALL}")
    agent.test(2, verbose=False)  # Reduced number of test episodes

    # Analyze results
    portfolios = agent.env.portfolios
    portfolios.set_index('date', inplace=True)

    # Prepare allocations DataFrame
    allocations_list = portfolios['xt'].tolist()
    # Filter out any missing or incomplete allocation dictionaries
    allocations_list = [alloc for alloc in allocations_list if isinstance(alloc, dict) and len(alloc) == len(SYMBOLS)]
    allocations = pd.DataFrame(allocations_list, index=portfolios.index[:len(allocations_list)])
    allocations = allocations.fillna(0)
    allocations = allocations[list(SYMBOLS)]

    allocations.plot(figsize=(8, 4), title='Asset Allocations Over Time', lw=1)
    plt.xlabel('Date')
    plt.ylabel('Allocation')
    plt.legend(loc='upper right', fontsize='small')
    plt.show()

    # Plot portfolio value
    portfolios['pv_new'].plot(title='Portfolio Value Over Time', lw=1)
    plt.xlabel('Date')
    plt.ylabel('Portfolio Value')
    plt.show()

    # Calculate performance metrics
    final_values = portfolios.groupby('e')['pv_new'].last()
    print(f"{Fore.MAGENTA}Average Final Portfolio Value: {final_values.mean():.4f}{Style.RESET_ALL}")

# Since we are in a Jupyter notebook, we can directly await the main function
await main()
